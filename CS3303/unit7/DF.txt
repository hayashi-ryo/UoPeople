Introduction

Buffer pool algorithms are essential techniques for improving disk I/O efficiency. Specifically, algorithms such as FIFO (First In First Out), LRU (Least Recently Used), and LFU (Least Frequently Used) are widely employed in data caching and memory management. These algorithms operate efficiently under different conditions, contributing to the optimization of data reading and writing. In this task, we execute these three algorithms using an online Java compiler and evaluate their characteristics and efficiency based on the results.
These algorithms are designed to address the issue of efficient management of cached data. When cache misses occur, they can severely impact the program’s execution speed, making the efficiency of buffer pool algorithms crucial. Efficient disk access is particularly necessary when loading large amounts of data from disk to memory (Shaffer, 2011). 
In the following sections, we will examine the behavior of FIFO, LRU, and LFU algorithms based on their execution results and discuss their advantages and disadvantages.

---

Explanation of FIFO Algorithm

1. Heuristic Explanation
The FIFO (First In First Out) algorithm is a simple method that prioritizes deleting the data that entered the buffer pool first. The fundamental operation of FIFO is to remove the oldest page first, holding the pages in the buffer according to the order they were added. Each time new data is entered into the buffer, the oldest data is removed. As a result, pages are always deleted based on the order they were added, without considering their access frequency.
In this simulation, the algorithm was executed with a buffer size of 2, a page count of 6, and the following page values: 5, 5, 5, 10, 20, 5. The execution result showed that the first three instances of 5 were added sequentially to the buffer, followed by the addition of new pages 10, 20, and finally 5. During this process, the oldest pages were deleted one by one, leaving 20 and 5 in the buffer at the end.

Figure 1: Execution results of the FIFO algorithm

2. Inefficient Conditions
Due to its simplicity, the FIFO algorithm can sometimes inefficiently delete frequently used data. In this example, the page value 5 was used multiple times, but the oldest 5 was deleted, resulting in inefficient operation. In such cases, frequently accessed pages are not retained, and cache misses are more likely to occur.
For example, if the value 5 is used very frequently, the FIFO algorithm will ignore this frequency and delete the first occurrence of 5, forcing the data to be read again from disk and reducing the buffer’s efficiency. Thus, FIFO often results in inefficient outcomes when high-frequency pages are deleted at certain points.

3. Efficient Conditions
There are also situations where FIFO operates efficiently. For example, when data is used sequentially, FIFO simplifies buffer management and avoids excessive page replacement. In particular, if data is rarely reaccessed after being read once, FIFO becomes an effective algorithm. This scenario is often observed in systems that employ data prefetching, where FIFO operates efficiently under sequential access patterns (Shaffer, 2011).

---

Explanation of LRU Algorithm

1. Heuristic Explanation
The LRU (Least Recently Used) algorithm efficiently manages the cache by deleting the page that has not been used for the longest time. This algorithm determines which page to delete based on its past usage, and since recently used pages remain in the cache, frequently accessed data is retained efficiently.
In this simulation, the algorithm was executed with a buffer size of 2, a page count of 6, and the following page values: 5, 10, 5, 10, 20, 5. The result showed that LRU always deleted the page that had not been used recently, leaving 5 and 10 in the buffer at the end. Since 5 was frequently used, it remained in the buffer for an extended period.

Figure 2: Execution results for the LRU algorithm

2. Efficient Conditions
The LRU algorithm works very efficiently when data frequently accessed is more likely to be retained in the buffer. For example, in this simulation, the value 5 was used multiple times, and LRU prioritized keeping this frequently used page in the buffer, which increased cache hit rates on subsequent accesses. As a result, disk accesses were reduced, leading to efficient data processing.
One real-world example is the caching system in web browsers. Resources such as images and CSS files are cached, with frequently accessed items kept in the cache while resources not accessed for a long time are deleted. In such situations, the LRU algorithm operates effectively and improves system performance (Shaffer, 2011).

3. Inefficient Conditions
However, LRU can also be inefficient in some scenarios. Specifically, when access patterns are random, or when a large amount of data is accessed in a short time, LRU frequently deletes pages that have not been recently used, leading to frequent cache replacements. This causes an increase in cache misses and, as a result, can degrade overall system performance.
For example, in cases where data is accessed randomly in large quantities over short periods, LRU may not perform well. In such situations, alternative buffer management algorithms may be more suitable.

---

Explanation of LFU Algorithm

1. Heuristic Explanation
The LFU (Least Frequently Used) algorithm prioritizes deleting the pages with the lowest access frequency within the buffer pool. By tracking each page's access frequency and removing the pages with the fewest accesses, the algorithm effectively keeps frequently used data in the buffer for extended periods.
In this simulation, two different inputs were used. In the first run, the buffer size was 2, the page count was 6, and the page values were 5, 10, 5, 10, 20, 5. In the second run, the page values were 5, 10, 20, 25, 30, 35. The LFU algorithm deleted the pages with the lowest frequency in both cases.
In the first run, the page value 5 was used most frequently, so it remained in the buffer for a long time, and the page 20 was deleted. In the second run, where all the page values had the same frequency of 1, the first added page (5) was deleted, and the subsequent pages were added one by one.

Figure 3: LFU algorithm, first execution
Figure 4: LFU algorithm, second execution

2. Behavior When Duplicate Pages Are Present
In the first input (5, 10, 5, 10, 20, 5), the page value 5 was used multiple times, making it the page with the highest frequency compared to others. The LFU algorithm deleted the page with the lowest frequency (20) and retained the most frequently used page (5) in the buffer.
Thus, when some pages are used multiple times, LFU prioritizes retaining the frequently used pages and deleting the less frequently used ones, keeping pages that are likely to be reaccessed in the buffer.

3. Behavior When No Duplicate Pages Are Present
In the second input (5, 10, 20, 25, 30, 35), where all pages were used only once, the LFU algorithm deleted the pages based on the order in which they were added. In this simulation, the first added page (5) was deleted, and the remaining pages were added sequentially to the buffer.
The buffer’s behavior was as follows:

- First, page 5 was added, followed by page 10. The result was "buffer :5 -1" → "buffer :5 10."
- Next, page 20 was added, and page 5 was deleted. The result was "buffer :20 10."
- Subsequently, pages 25, 30, and 35 were added, with the page in the first buffer being deleted each time, leaving "buffer :35 10" at the end.
This behavior demonstrates that when all pages have the same frequency, the LFU algorithm deletes the page that was added first.

4. Efficient Conditions
LFU is highly efficient in scenarios where certain data is used more frequently than others. In cases like database access or cache management, where frequently accessed data should be retained, LFU allows for efficient memory management by deleting rarely used data. In this simulation, it was confirmed that frequently accessed page 5 remained in the buffer, minimizing disk accesses and ensuring efficient operation (Shaffer, 2011).

5. Inefficient Conditions
However, LFU is not suitable for all situations. Particularly when data is accessed randomly or all pages are accessed equally, LFU can become inefficient. In the second run of the simulation, where all pages were used only once, the first added page was deleted one after another. In such cases, LFU behaves similarly to FIFO, resulting in reduced efficiency.

---

Conclusion

Through this simulation, we confirmed that each of the buffer pool algorithms—FIFO, LRU, and LFU—has distinct characteristics, and the optimal choice depends on the scenario. FIFO operates efficiently in situations where sequential processing is required but can result in frequently used data being deleted easily. LRU efficiently retains frequently accessed data by deleting the least recently used data, making it highly effective when access patterns are regular.
On the other hand, LFU prioritizes retaining frequently used data but may be inefficient when all data is accessed equally or when certain data is accessed only once over short periods. In such scenarios, LFU behaves similarly to FIFO, leading to a decrease in efficiency.
In summary, each algorithm demonstrates strengths under specific conditions, and selecting the appropriate one depends on the operating environment and access patterns. For example, LRU or LFU is effective when dealing with frequently reused data, while FIFO may be more suitable for sequential processing. Ultimately, efficient buffer management requires selecting the algorithm based on system characteristics and data access patterns.


References
Shaffer, C. (2011). A Practical Introduction to Data Structures and Algorithm Analysis. Blacksburg: Virginia. Tech.
paiza. (n.d.). Online Java compiler and coding environment. paiza.io. Retrieved October 1, 2024, from https://paiza.io/